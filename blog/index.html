<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <title>Blog | Urielle-AI</title>
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <meta name="description" content="Weekly insights on AI governance, safety, incentives, and adversarial risk thinking." />

  <link rel="stylesheet" href="../assets/css/styles.css" />
  <link rel="preconnect" href="https://fonts.googleapis.com" />
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
  <link href="https://fonts.googleapis.com/css2?family=Space+Grotesk:wght@300;400;500;600;700&display=swap" rel="stylesheet" />
</head>

<body>
  <header class="site-header">
    <!-- Make logo clickable & consistent -->
    <a href="../index.html" class="logo">
      <span class="logo-mark">⛨</span>
      <span class="logo-text">Urielle-AI</span>
    </a>

    <nav class="nav">
      <a href="../index.html#hero">Home</a>
      <a href="index.html" class="active">Blog</a>
      <a href="../services.html">Services</a>
      <a href="../index.html#portfolio">Portfolio</a>
      <a href="../index.html#ai-thara">AI-Thara</a>
      <a href="../index.html#about">About</a>
      <a href="../index.html#contact" class="nav-cta">Contact Us</a>
    </nav>
  </header>

  <main>
    <section class="section">
      <div class="section-header">
        <h2>Weekly Updates</h2>
        <p>A weekly series on AI failure modes, incentives, and governance blind spots.</p>
      </div>
  
  <!-- PHASE 1 -->
  <section class="phase">
    <div class="phase-head">
      <div class="phase-kicker">PHASE 1 (Days 1–30)</div>
      <h3 class="phase-title">Failure-Mode Thinking Foundations</h3>
      <p class="phase-desc">Goal: Rewire how you think about AI risk — failure-first, system-level.</p>
    </div>

    <div class="weekly-grid">
      <!-- Week 4 -->
        <article class="card card-outline">
          <h3>Human-in-the-Loop Illusions: Why Oversight Often Fails When It Matters Most</h3>
          <p>
            Human Approves Is Not the Same as A Human Controls It
          </p>
          <p class="card-tagline">Week 4 • Human-in-the-loop Oversight• Proxy metrics</p>
          <a class="btn btn-ghost" href="posts/2026-01-09-Human-in-the-Loop Illusions-Why-Oversight-Often-Fails.html">Read</a>
        </article>
        
        <!-- Week 3 -->
        <article class="card card-outline">
          <h3>Specification Gaming & Proxy Metrics Failure</h3>
          <p>
            When AI systems learn to optimize the metric instead of the intent,
            success itself becomes a failure mode.
          </p>
          <p class="card-tagline">Week 3 • Specification gaming • Proxy metrics</p>
          <a class="btn btn-ghost" href="posts/2026-01-02-Specification-Gaming-and-Proxy-Metrics-Failure.html">Read</a>
        </article>

        <!-- Week 2 -->
        <article class="card card-outline">
          <h3>Why AI Systems Fail — Even When They Do What We Ask</h3>
          <p>When AI systems succeed at scale, harm can emerge from misalignment—not bugs.</p>
          <p class="card-tagline">Week 2 • Alignment failure • Enterprise risk</p>
          <a class="btn btn-ghost" href="posts/2025-12-26-why-ai-systems-fail-when-they-work.html">Read</a>
        </article>

        <!-- Week 1 -->
        <article class="card card-outline">
          <h3>What AI Optimizes — and Why That Matters</h3>
          <p>Why incentives and proxy goals matter more than intent when AI scales.</p>
          <p class="card-tagline">Week 1 • Incentives • Human impact</p>
          <a class="btn btn-ghost" href="posts/2025-12-19-what-this-ai-optimizes.html">Read</a>
        </article>
       </div>
      </section>

      <!-- PHASE 2 (placeholder) -->
      <section class="phase">
        <div class="phase-head">
          <div class="phase-kicker">PHASE 2 (Days 31–60)</div>
          <h3 class="phase-title">Adversarial & Existential Risk</h3>
          <p class="phase-desc">Goal: Think like an attacker, not a regulator.</p>
        </div>

        <div class="weekly-grid">
          <!-- Week 5+ cards go here -->
          <article class="card card-outline">
            <h3>Why Adversarial AI Risk Is Not a Cybersecurity Problem</h3>
            <p>AI systems fail strategically under opposition. Security fixes don’t scale against adaptive attackers.</p>
            <p class="card-tagline">Week 5 • Adversarial ML • Exploitability</p>
            <a class="btn btn-ghost" href="posts/2026-01-16-why-adversarial-ai-risk-is-not-cybersecurity.html">Read</a>
          </article>
        </div>

        <div class="weekly-grid">
          <!-- Week 6+ cards go here --> 
          <article class="card card-outline">
            <h3>AI Agents as Goal-Pursuing Entities</h3>
            <p>AI agents, goal pursuit, tool use, and why alignment gets harder as systems gain agency.</p> 
            <p class="card-tagline">Week 6 • AI Agents.Goal-Pursuing Entities</p> 
            <a class="btn btn-ghost" href="posts/2026-01-23-ai-agents-goal-pursuing-entities.html">Read</a> 
          </article>
        </div>


        
      </section>

    </section>
  </main>
    
  <footer class="site-footer">
    <p>© <span id="year"></span> Urielle-AI. Where innovation meets accountability in AI.</p>
  </footer>

  <script>
    document.getElementById('year').textContent = new Date().getFullYear();
  </script>
</body>
</html>
