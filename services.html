<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <title>Services | Urielle-AI</title>

  <!-- Link to your existing CSS -->
  <link rel="stylesheet" href="assets/css/style.css">

  <meta name="description" content="Human-centric and adversarial AI governance services aligned with the EU AI Act, AI Verify, and global AI safety standards.">
</head>

<body>

  <!-- ===== Header / Navigation ===== -->
  <header class="site-header">
    <div class="container">
      <h1 class="logo">Urielle-AI</h1>
      <nav class="nav">
        <a href="index.html">Home</a>
        <a href="services.html" class="active">Services</a>
      </nav>
    </div>
  </header>

  <!-- ===== Hero Section ===== -->
  <section class="hero">
    <div class="container">
      <h2>Human-Centric & Adversarial AI Governance</h2>
      <p>
        We help organizations govern, audit, and stress-test AI systems
        by assuming both <strong>human impact risks</strong> and
        <strong>adversarial system behavior</strong>.
      </p>
    </div>
  </section>

  <!-- ===== Services Content ===== -->
  <main class="content">
    <div class="container">

      <!-- Section 1 -->
      <section class="service">
        <h3>1. EU AI Act Readiness & Compliance Advisory</h3>
        <p>
          We support organizations preparing for or operating under the
          EU AI Act through practical, risk-based governance.
        </p>
        <ul>
          <li>AI system risk classification</li>
          <li>Gap analysis against EU AI Act Titles III & IV</li>
          <li>Governance roles, accountability & oversight design</li>
          <li>Compliance roadmap and documentation guidance</li>
        </ul>
        <p class="emphasis">
          Compliance is treated as a baseline — not a guarantee of safety.
        </p>
      </section>

      <!-- Section 2 -->
      <section class="service">
        <h3>2. AI Risk & Failure-Mode Assessment (Beyond Compliance)</h3>
        <p>
          Traditional audits ask whether requirements are met.
          We examine how systems could fail even when requirements are satisfied.
        </p>
        <ul>
          <li>Failure-mode and misuse analysis</li>
          <li>Incentive and goal-misalignment risks</li>
          <li>Emergent and unexpected behavior scenarios</li>
          <li>Automation bias and over-reliance risks</li>
        </ul>
      </section>

      <!-- Section 3 -->
      <section class="service">
        <h3>3. Human Oversight & Control Design</h3>
        <p>
          The EU AI Act assumes humans can intervene.
          We test whether this assumption holds in real operational conditions.
        </p>
        <ul>
          <li>Human-in-the-loop effectiveness evaluation</li>
          <li>Override, escalation & shutdown feasibility</li>
          <li>Cognitive load and interface risk analysis</li>
          <li>False sense of control scenarios</li>
        </ul>
      </section>

      <!-- Section 4 -->
      <section class="service">
        <h3>4. Transparency, Explainability & Trust Controls</h3>
        <p>
          Transparency builds trust — but only when it is honest and appropriate.
        </p>
        <ul>
          <li>Explainability aligned with system risk and user role</li>
          <li>User disclosure design (EU AI Act Article 52)</li>
          <li>Explainability vs controllability assessment</li>
          <li>Avoidance of “explainability theater”</li>
        </ul>
      </section>

      <!-- Section 5 -->
      <section class="service">
        <h3>5. Post-Market Monitoring & Continuous AI Assurance</h3>
        <p>
          AI risk does not end at deployment.
        </p>
        <ul>
          <li>Post-market monitoring design (Article 61)</li>
          <li>Incident detection and reporting workflows</li>
          <li>Model, data, and behavior drift monitoring</li>
          <li>Governance updates for evolving systems</li>
        </ul>
        <p class="emphasis">
          We monitor not only performance drift, but behavioral and incentive drift.
        </p>
      </section>

      <!-- Section 6 -->
      <section class="service">
        <h3>6. AI Governance Training & Executive Briefings</h3>
        <p>
          For leaders who need clarity without hype.
        </p>
        <ul>
          <li>Executive and board-level AI risk briefings</li>
          <li>AI governance and compliance training</li>
          <li>Human-centric and adversarial risk awareness</li>
          <li>Regulatory expectations vs technical realities</li>
        </ul>
      </section>

      <!-- Philosophy -->
      <section class="service philosophy">
        <h3>Our Governance Philosophy</h3>
        <p>
          Urielle-AI operates on three principles:
        </p>
        <ul>
          <li><strong>Human-Centric</strong> — AI must respect human agency</li>
          <li><strong>Adversarial-Aware</strong> — assume systems can be exploited</li>
          <li><strong>Regulator-Ready</strong> — governance must stand up to scrutiny</li>
        </ul>
        <p class="emphasis">
          We work at the intersection of ethics, engineering, and governance.
        </p>
      </section>

    </div>
  </main>

  <!-- ===== Footer ===== -->
  <footer class="site-footer">
    <div class="container">
      <p>© 2025 Urielle-AI. Independent AI Governance & Safety Advisory.</p>
    </div>
  </footer>

</body>
</html>
